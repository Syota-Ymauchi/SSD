{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d7793a-1b1d-48e3-8cf2-35872e65d953",
   "metadata": {
    "id": "a5d7793a-1b1d-48e3-8cf2-35872e65d953",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torchvision.ops import box_iou # IoU計算\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebe619-3fdd-4a26-abac-13dfc81d4bbd",
   "metadata": {
    "id": "b8ebe619-3fdd-4a26-abac-13dfc81d4bbd"
   },
   "source": [
    "### vggモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5598949-8ef2-4d2a-9cfe-5badb27df966",
   "metadata": {
    "id": "d5598949-8ef2-4d2a-9cfe-5badb27df966",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_vgg():\n",
    "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C',\n",
    "           512, 512, 512, 'M', 512, 512, 512]\n",
    "    layers = []\n",
    "    in_chanels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2)] #出力サイズを切り捨て(デフォルトでeil_mode=False)\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, ceil_mode=True)] # 出力サイズを切り上げる\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_chanels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU()]\n",
    "            in_chanels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6, nn.ReLU(), conv7, nn.ReLU()]\n",
    "\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a709a49-8865-44f5-a871-30727ded823b",
   "metadata": {
    "id": "6a709a49-8865-44f5-a871-30727ded823b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6660d5d6-52c8-496c-b706-156669af9692",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6660d5d6-52c8-496c-b706-156669af9692",
    "outputId": "c287c629-cd17-4d6f-ced6-9bfb326bfde3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = [3]\n",
    "c = [4]\n",
    "a += b\n",
    "a +=c\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04a8fef-f64a-4e6c-9e7d-3160956a59ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e04a8fef-f64a-4e6c-9e7d-3160956a59ff",
    "outputId": "8f2aad48-662a-4685-a951-c98253a6e949",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3]+[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e504e64-e3c5-4b1a-a4b0-3bed8780f9ca",
   "metadata": {
    "id": "1e504e64-e3c5-4b1a-a4b0-3bed8780f9ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_chanels = 3\n",
    "v = 512\n",
    "layers = []\n",
    "layers += [nn.MaxPool2d(kernel_size=2)]\n",
    "pool1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "conv2 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "layers += [pool1, conv1, nn.ReLU(), conv2, nn.ReLU()]\n",
    "conv2d = nn.Conv2d(in_chanels, v, kernel_size=3, padding=1)\n",
    "layers += [conv2d, nn.ReLU()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c761abdc-4d35-40db-811f-976d21531a2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c761abdc-4d35-40db-811f-976d21531a2d",
    "outputId": "9863396b-9ed6-4be4-ea66-47088107f0d5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False),\n",
       " Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6)),\n",
       " ReLU(),\n",
       " Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1)),\n",
       " ReLU(),\n",
       " Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e99ca2-5bdc-4c30-b2eb-5bf7a0fdf007",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8e99ca2-5bdc-4c30-b2eb-5bf7a0fdf007",
    "outputId": "b5be1106-65a5-4404-b097-f5ab056353b4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.weight\n",
      "4.weight\n",
      "6.weight\n"
     ]
    }
   ],
   "source": [
    "module = nn.ModuleList(layers)\n",
    "for name, params in module.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f11e5b1-b7c2-4a1c-9278-02b0d31b211a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f11e5b1-b7c2-4a1c-9278-02b0d31b211a",
    "outputId": "63bd75c8-bd8c-4507-9023-08611fa3940d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList(layers)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bbfe3-ef6e-4609-a4be-d61c44de1c1c",
   "metadata": {
    "id": "3d2bbfe3-ef6e-4609-a4be-d61c44de1c1c"
   },
   "source": [
    "### extrasモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def03c29-66c0-4adb-a2be-a933963bd9cf",
   "metadata": {
    "id": "def03c29-66c0-4adb-a2be-a933963bd9cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_extras():\n",
    "    layers = [\n",
    "        # out3\n",
    "        nn.Conv2d(1024, 256, kernel_size=1),\n",
    "        nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "        # out3からout4\n",
    "        nn.Conv2d(512, 128, kernel_size=1),\n",
    "        nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "        # out4からout5\n",
    "        nn.Conv2d(256, 128, kernel_size=1),\n",
    "        nn.Conv2d(128, 256, kernel_size=3),\n",
    "        # out6からout6\n",
    "        nn.Conv2d(256, 128, kernel_size=1),\n",
    "        nn.Conv2d(128, 256, kernel_size=3),\n",
    "    ]\n",
    "    return nn.ModuleList(layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe90a5f-37fd-442e-8021-64a48aadbc24",
   "metadata": {
    "id": "cfe90a5f-37fd-442e-8021-64a48aadbc24"
   },
   "source": [
    "### Locモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cc6a2c-764d-48d0-92cb-98262a0ef70a",
   "metadata": {
    "id": "a3cc6a2c-764d-48d0-92cb-98262a0ef70a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_loc(num_classes=21):\n",
    "    \"\"\"\n",
    "    オフセットの予測を出力する\n",
    "    nn.Conv2d第2引数は出力ベクトルのchanel方向の次元数.作成されるDBox(4 or 6個)ごとにオフセットを出力するので\n",
    "    作成されるオフセットの数*4となる\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(512, 4*4, kernel_size=3, padding=1),\n",
    "\n",
    "        # out2に対する処理\n",
    "        nn.Conv2d(1024, 6*4, kernel_size=3, padding=1),\n",
    "\n",
    "        # out3に対する処理\n",
    "        nn.Conv2d(512, 6*4, kernel_size=3, padding=1),\n",
    "\n",
    "        # out4に対する処理\n",
    "        nn.Conv2d(256, 6*4, kernel_size=3, padding=1),\n",
    "\n",
    "        # out5に対する処理\n",
    "        nn.Conv2d(256, 4*4, kernel_size=3, padding=1),\n",
    "\n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(256, 4*4, kernel_size=3, padding=1)\n",
    "    ]\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7f26a-254b-444c-bba7-6450d0ea47d7",
   "metadata": {
    "id": "50b7f26a-254b-444c-bba7-6450d0ea47d7"
   },
   "source": [
    "### confモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0177d609-f3fc-4b71-9829-6479c60f1a89",
   "metadata": {
    "id": "0177d609-f3fc-4b71-9829-6479c60f1a89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_coef(num_classes=21):\n",
    "    \"\"\"\n",
    "    クラスの予測を出力する\n",
    "    nn.Conv2d第2引数は出力ベクトルの次元数.作成されるDBox(4 or 6個)ごとに各クラスの信頼度を出力するので\n",
    "    作成されるオフセットの数*num_classesとなる\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(512, 4*num_classes, kernel_size=3, padding=1), # [b, 16, 38, 38]\n",
    "\n",
    "        # out2に対する処理\n",
    "        nn.Conv2d(1024, 6*num_classes, kernel_size=3, padding=1),\n",
    "\n",
    "        # out3に対する処理\n",
    "        nn.Conv2d(512, 6*num_classes, kernel_size=3, padding=1),\n",
    "\n",
    "        # out4に対する処理\n",
    "        nn.Conv2d(256, 6*num_classes, kernel_size=3, padding=1),\n",
    "\n",
    "        # out5に対する処理\n",
    "        nn.Conv2d(256, 4*num_classes, kernel_size=3, padding=1),\n",
    "\n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(256, 4*num_classes, kernel_size=3, padding=1)\n",
    "    ]\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0772b-33a5-4443-890f-280efc1824cb",
   "metadata": {
    "id": "d4a0772b-33a5-4443-890f-280efc1824cb"
   },
   "source": [
    "### L2Normの実装\n",
    "### Layer Normalizationの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d916f6-3308-40d6-ac78-e70cc434051e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7d916f6-3308-40d6-ac78-e70cc434051e",
    "outputId": "69dc3cc6-6391-4e29-e404-b2ab5232b8a1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9592e-04, 1.4391e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf25be18-dc01-40b0-9d12-ee330ca5e710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf25be18-dc01-40b0-9d12-ee330ca5e710",
    "outputId": "f4206ace-77c6-4294-a6fc-6d2413301636",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18466715-c13b-4a33-8228-01dc43edf4fa",
   "metadata": {
    "id": "18466715-c13b-4a33-8228-01dc43edf4fa"
   },
   "source": [
    "###### nn.Parameterは、PyTorchのモデル内で学習可能なパラメータを定義するために使用されるクラスです。主に、ニューラルネットワークの層で使用され、バックプロパゲー ションを通じて最適化される重みやバイアスを定義します。\n",
    "\n",
    "###### nn.init.constant_は、PyTorchの初期化モジュールで提供される関数の一つで、テンソルのすべての要素を指定した定数で初期化するために使用されます。この関数は、通常、モデルのパラメータの初期値を設定する際に利用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee5882-a1d8-4b1a-a54b-153afac1aae4",
   "metadata": {
    "id": "f6ee5882-a1d8-4b1a-a54b-153afac1aae4"
   },
   "source": [
    "#### L2Norm : チャネル方向のL2Normの合計を1にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8c49a2c-7a4b-44a5-9e12-f9a66d99ab7e",
   "metadata": {
    "id": "e8c49a2c-7a4b-44a5-9e12-f9a66d99ab7e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, n_channels=512, scale=20):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale # 正規化後に掛けるパラメータ,channel分だけある.(これはbackwaradで最適化される)\n",
    "        self.eps = 1e-10 # 0で割ることを防ぐためのε\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        nn.init.constant_(self.weight, self.gamma) # self.gamma(デフォルトで20)でweightを初期化\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X : [b * c * h * w]を想定\n",
    "        \"\"\"\n",
    "        norm = X.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps # norm : [b, 1, h, w]\n",
    "\n",
    "        # 入力をnormで割る\n",
    "        X = torch.div(X, norm) # X : [b, c, h, w]\n",
    "\n",
    "        # スケーリングの重みを掛ける\n",
    "        out = self.weight.reshape(1, self.n_channels, 1, 1) * X # self.weight.reshape(1, self.n_channels, 1, 1) : [1, c, 1, 1]\n",
    "                                                                # out : [b, c, h, w]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1301af03-6dff-4dcb-b645-892044248df5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1301af03-6dff-4dcb-b645-892044248df5",
    "outputId": "b49d8ffd-5bc2-4fb4-fd91-6c772eb4138f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = nn.Parameter(torch.Tensor(3))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c27c410b-0e5a-4bb4-a5a4-93e529fc27d9",
   "metadata": {
    "id": "c27c410b-0e5a-4bb4-a5a4-93e529fc27d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.Tensor(3))\n",
    "eps = 1e-10\n",
    "X = torch.randn((8, 3, 8, 8))\n",
    "norm = X.pow(2).sum(dim=1, keepdim=True).sqrt() + eps\n",
    "\n",
    "X = torch.div(X, norm) # これで各ピクセルの和は1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0264f63b-9a8e-407d-8e77-aa4ee55e9dca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0264f63b-9a8e-407d-8e77-aa4ee55e9dca",
    "outputId": "4caf54d7-1160-4cba-d888-40d3356f9fe1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 8, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.shape # データ1つの正規化(channel間での正規化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46da56a9-eba7-42bd-87ac-c37097ebb23a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46da56a9-eba7-42bd-87ac-c37097ebb23a",
    "outputId": "c8626991-2b85-4638-dc2f-3e9d82933bbb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 8, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b56c71-53bd-46fd-a873-4d3a955f1a6c",
   "metadata": {
    "id": "a6b56c71-53bd-46fd-a873-4d3a955f1a6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(X) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "004b53a3-10dc-4351-a9fd-439e0898e696",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "004b53a3-10dc-4351-a9fd-439e0898e696",
    "outputId": "301d660a-9543-405b-9767-161cdce8e1fd",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 8, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd2e20-d6a3-4305-b77d-08ba4c848cf1",
   "metadata": {
    "id": "0ffd2e20-d6a3-4305-b77d-08ba4c848cf1"
   },
   "source": [
    "### DBoxの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a7d1f-c68a-4873-b20a-20027473d57c",
   "metadata": {
    "id": "f28a7d1f-c68a-4873-b20a-20027473d57c"
   },
   "source": [
    "###### self.min_sizes = [30, 60, 111, 162, 213, 264]\n",
    "###### self.max_sizes = [60, 111, 162, 213, 264, 315]\n",
    "###### この値から、解像度が38*38の特徴マップでは画像の縦、横10% ~ 20%の大きさの画像の検出が得意ということ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a7fa67b-5923-4f53-9b33-a74f80eab183",
   "metadata": {
    "id": "3a7fa67b-5923-4f53-9b33-a74f80eab183",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PriorBox:\n",
    "    def __init__(self):\n",
    "        self.image_size = 300 # 入力画像のサイズを300 × 300と想定\n",
    "        # 解像度が38 : 1つの特徴量マップでは300/38で7ピクセル分の情報を表現\n",
    "        self.feature_maps = [38, 19, 10, 5, 3, 1]\n",
    "        self.steps = [8, 16, 32, 64, 100, 300] # 特徴量マップの1セルが何ピクセルを(ピクセル/セル)表現するかをリストに格納.32, 64は計算の効率性のため少しずらした値(2の累乗)を設定\n",
    "                                               # 例えば300/38 ≒ 8, 300/19 ≒ 16としている\n",
    "        self.min_sizes = [30, 60, 111, 162, 213, 264] # 30 ... 画像の10%程度の大きさの物体の検出に適している\n",
    "\n",
    "        self.max_sizes = [60, 111, 162, 213, 264, 315] # 60 ...画像の20%程度の大きさの物体の検出に適している\n",
    "        self.aspect_rations = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "\n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps): # [38, 19, 10, 5, 3, 1]\n",
    "            for i, j in product(range(f), repeat=2): # 各特徴量マップのセルごとにDBox作成\n",
    "                #self.steps は計算効率のために調整されたセルのピクセル数を格納しているが、\n",
    "                # 実際のDBoxの配置で精度を保つために、f_k = self.image_size / self.steps[k] で再度スケールを計算\n",
    "                f_k = self.image_size / self.steps[k] # 特徴量マップf_k個で1になる\n",
    "                cx = (j + 0.5) / f_k # 比の計算 これを座標としている\n",
    "                cy = (i + 0.5) / f_k\n",
    "                s_k = self.min_sizes[k] / self.image_size # これは最小サイズの正方形のサイズ. 特徴量マップの解像度で固定\n",
    "                mean += [cx, cy, s_k, s_k] # 最小サイズの正方形のDBox作成\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) # これは最大サイズの正方形のサイズ. 特徴量マップの解像度で固定\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime] # 最大サイズの正方形のDBox作成\n",
    "                for ar in self.aspect_rations[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        # イメージ : [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4]\n",
    "        # ===>\n",
    "        # tensor([[1., 2., 3., 4.],\n",
    "        # [5., 6., 7., 8.],\n",
    "        # [1., 2., 3., 4.]])\n",
    "\n",
    "        output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a294e3a-60bf-4b77-9a62-e14c588140d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a294e3a-60bf-4b77-9a62-e14c588140d3",
    "outputId": "cd219128-4fd7-448a-e7e1-de30c572a136",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7eeeb794-c18f-4c07-97bf-f7e5bad20af4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7eeeb794-c18f-4c07-97bf-f7e5bad20af4",
    "outputId": "4c75c1fe-a99a-467a-e366-3e354326e3c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# from itertools import product\n",
    "for i, j in product(range(3), range(4)): # 直積を計算\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec102f09-8bf6-4d0e-b672-0dc5b72d2e69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec102f09-8bf6-4d0e-b672-0dc5b72d2e69",
    "outputId": "bcaab93c-8c39-4a4e-fd46-4e3199e06239",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "for i, j in product(range(3), repeat=2):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "155be06b-d720-45d2-a895-8ae4f26288d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "155be06b-d720-45d2-a895-8ae4f26288d1",
    "outputId": "ec78382e-3b80-4708-da4d-5a307be11cdf",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = []\n",
    "mean += [1,2,3,4]\n",
    "mean += [5,6,7,8]\n",
    "mean += [1,2,3,4]\n",
    "# エラーになる\n",
    "# mean += [1,2,3]\n",
    "\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18172f55-a14e-4a68-8842-2e75c4d1e196",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18172f55-a14e-4a68-8842-2e75c4d1e196",
    "outputId": "f301cd9a-b20e-4096-da05-6b47fe055efa",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.],\n",
       "        [1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(mean).view(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34588fef-205a-4151-8eee-b664dfa37b62",
   "metadata": {
    "id": "34588fef-205a-4151-8eee-b664dfa37b62"
   },
   "source": [
    "### SSDのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43de89ef-f9f8-4036-9cd3-cd149d89e2e5",
   "metadata": {
    "id": "43de89ef-f9f8-4036-9cd3-cd149d89e2e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self, phase='train', num_classes=21):\n",
    "        super().__init__()\n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        self.vgg = make_vgg()\n",
    "        self.extras = make_extras()\n",
    "        self.L2Norm = L2Norm()\n",
    "        self.loc = make_loc()\n",
    "        self.conf = make_coef()\n",
    "        dbox = PriorBox()\n",
    "        self.priors = dbox.forward() # self.priorsには各解像度の各セルに対してのDBoxが4or6個格納されている\n",
    "\n",
    "        if phase == 'test':\n",
    "            self.detect = Dtect()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X : [b, c, h=300, w=300]\n",
    "        \"\"\"\n",
    "        bs = X.shape[0]\n",
    "        # lout = []  各セルのオフセットの予測が格納\n",
    "        # cout = []  各セルのクラス分類の結果が格納\n",
    "        # out = []  各解像度の特徴マップが出力\n",
    "        out, lout, cout = [], [], []\n",
    "        for i in range(23): # 23はvggの定義でL2Normが適用されるまでに通過する層数(Conv2d, ReLU, Maxpool2d)\n",
    "            X = self.vgg[i](X)\n",
    "        X1 = X\n",
    "        out.append(self.L2Norm(X1)) # out1を得る\n",
    "\n",
    "        for i in range(23, len(self.vgg)):\n",
    "            X = self.vgg[i](X)\n",
    "\n",
    "        out.append(X) # out2を得る\n",
    "\n",
    "        # out3,4,5,6\n",
    "        for i in range(0, 8, 2):\n",
    "            X = F.relu(self.extras[i](X))\n",
    "            X = F.relu(self.extras[i+1](X))\n",
    "            out.append(X)\n",
    "\n",
    "        # オフセットとクラス毎の信頼度を求める\n",
    "        for i in range(6): # out1~out6に対する出力処理\n",
    "            # 各セルのオフセットの予測\n",
    "            lx = self.loc[i](out[i]).permute(0,2,3,1).reshape(bs, -1, 4)\n",
    "            # self.loc[i](out[i]).permute(0,2,3,1) : [bs, 38, 38, 16] reshape後 : [bs, 38*38*4, 4]になるのでは...\n",
    "            # 書籍では[bs, 38*38, 4] と各セルに対してオフセットが得られるとあるが...多分誤植\n",
    "\n",
    "            # cout = []  各セルのクラス分類を予測\n",
    "            # self.conf[i](out[i]) : [b, 4(or6)*num_classes, h, w]\n",
    "            # .permute(0,2,3,1) : [b, h, w, 4(or6)*num_classes]\n",
    "            # .reshape() : [b, h*w*4(or6), num_classes]\n",
    "            cx = self.conf[i](out[i]).permute(0,2,3,1).reshape(bs, -1, self.num_classes)\n",
    "            lout.append(lx)\n",
    "            cout.append(cx)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        lout = torch.cat(lout, 1) # [bs, 38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4, 4] # 1枚の画像の38*38の各セルに対して4つのDBoxがあり,4次元のオフセットがある\n",
    "        cout = torch.cat(cout, 1) # [bs, 38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4, self.num_classes] # 1枚の画像の38*38の各セルに対して4つのDBoxがあり, 21クラスのクラス分類を行う\n",
    "        outputs = (lout, cout, self.priors)\n",
    "        if self.phase == 'test':\n",
    "            return self.detect.apply(output, self.num_classes)\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90da8690-0bae-4c96-92eb-cb96080df955",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90da8690-0bae-4c96-92eb-cb96080df955",
    "outputId": "e306c52a-79bf-4a4c-aa7c-33de66e6246f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7468ef6-cb2c-4b73-bce4-bf3e587e4973",
   "metadata": {
    "id": "d7468ef6-cb2c-4b73-bce4-bf3e587e4973",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(8, 3, 300, 300)\n",
    "test_model = SSD()\n",
    "lout, cout, priors = test_model(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d619c7e-4e7e-4e9e-a74b-5b34e7c2fb6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d619c7e-4e7e-4e9e-a74b-5b34e7c2fb6c",
    "outputId": "352857f6-ab58-4480-c3f7-866298fafd8b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8732, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各セルにおけるオフセット\n",
    "lout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8648d3a3-4b9f-4a50-b279-5f77b8a25cfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8648d3a3-4b9f-4a50-b279-5f77b8a25cfc",
    "outputId": "9720bf49-abec-4136-92cd-76c4b1305534",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8732, 21])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各セルにおけるクラス分類\n",
    "cout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d27b0912-6dc9-4258-ba92-fe5da24810a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8732, 21])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cout.view(cout.size()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc6bca15-3b0c-4b4b-833f-fd66f36c0f91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc6bca15-3b0c-4b4b-833f-fd66f36c0f91",
    "outputId": "8132ab12-632d-43bc-cb27-15be6edd083c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8732, 4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各セルにおけるDBox\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78f98f43-ded7-42db-b8bc-dce7226bc7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8YQMirQNpe_A",
   "metadata": {
    "id": "8YQMirQNpe_A"
   },
   "source": [
    "## 損失関数の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffd16622-a126-494a-b346-111acd1283ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOC_CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', \n",
    "               'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10dbea90-8fe2-4f3e-9a4a-efa39e8f62e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOC_CLASSES.index('bicycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7040d6b2-27a6-40f3-9bc9-38eb0ba76a26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSDLoss(nn.Module):\n",
    "    def __init__(self, num_classes, priors, neg_pos_ratio=3, alpha=1.0, iou_threshold=0.5, device='cpu'):\n",
    "        super(SSDLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.priors = priors  # 事前定義されたデフォルトボックス\n",
    "        self.neg_pos_ratio = neg_pos_ratio  # Negative DBoxのサンプル数はPositive DBoxのサンプル数のneg_pos_ratio倍\n",
    "        self.alpha = alpha  # ロケーション(loc)損失の重み\n",
    "        self.iou_threshold = iou_threshold  # IoUの閾値\n",
    "\n",
    "    def forward(self, loc_preds, cls_preds, annotations):\n",
    "        \"\"\"\n",
    "        cls_preds: クラス予測値 (B, N, num_classes)\n",
    "        loc_preds: オフセット予測値 (B, N, 4)\n",
    "        annotations: 各画像のPASCAL VOC形式のアノテーションリスト\n",
    "        \"\"\"\n",
    "        # ターゲットを生成\n",
    "        cls_targets, loc_targets = self.create_targets(annotations)\n",
    "        cls_targets = cls_targets.to(self.device)\n",
    "        loc_targets = loc_targets.to(self.device)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        # クラス分類損失 (Cross-entropy)\n",
    "        cls_loss = self.compute_cls_loss(cls_preds, cls_targets)\n",
    "\n",
    "        # ボックス回帰損失 (Smooth L1 Loss)\n",
    "        loc_loss = self.compute_loc_loss(loc_preds, loc_targets, cls_targets)\n",
    "\n",
    "        # 合計損失\n",
    "        total_loss = cls_loss + self.alpha * loc_loss\n",
    "        \n",
    "        return total_loss, cls_loss, loc_loss\n",
    "\n",
    "    def create_targets(self, annotations):\n",
    "        \"\"\"\n",
    "        アノテーションからターゲットを作成\n",
    "        \"\"\"\n",
    "        batch_size = len(annotations)\n",
    "        cls_targets = torch.zeros(batch_size, len(self.priors), dtype=torch.long)\n",
    "        loc_targets = torch.zeros(batch_size, len(self.priors), 4)\n",
    "\n",
    "        for i, annotation in enumerate(annotations):\n",
    "            # 全てのバッチで共通にデフォルトボックスであるので self.priors[i]の必要はない\n",
    "            cls_targets[i], loc_targets[i] = self.generate_single_target(annotation, self.priors)\n",
    "\n",
    "        return cls_targets, loc_targets\n",
    "\n",
    "    def generate_single_target(self, annotation, default_boxes):\n",
    "        \"\"\"\n",
    "        1つの画像に対してターゲットを生成する\n",
    "        \"\"\"\n",
    "        img_width = int(annotation['annotation']['size']['width'])\n",
    "        img_height = int(annotation['annotation']['size']['height'])\n",
    "        cls_targets = torch.zeros(len(default_boxes), dtype=torch.long)\n",
    "        loc_targets = torch.zeros((len(default_boxes), 4))\n",
    "\n",
    "        for obj in annotation['annotation']['object']:\n",
    "            class_name = obj['name']\n",
    "            class_id = VOC_CLASSES.index(class_name)\n",
    "\n",
    "            # バウンディングボックスの正規化\n",
    "            xmin = float(obj['bndbox']['xmin']) / img_width\n",
    "            ymin = float(obj['bndbox']['ymin']) / img_height\n",
    "            xmax = float(obj['bndbox']['xmax']) / img_width\n",
    "            ymax = float(obj['bndbox']['ymax']) / img_height\n",
    "            gt_box = torch.tensor([[xmin, ymin, xmax, ymax]]) # box_iouは2次元で渡す必要がある\n",
    "\n",
    "            # IoUの計算\n",
    "            ious = box_iou(gt_box, default_boxes)[0]\n",
    "            pos_idx = ious > self.iou_threshold\n",
    "            cls_targets[pos_idx] = class_id\n",
    "            loc_targets[pos_idx] = self.encode_offsets(default_boxes[pos_idx], gt_box)\n",
    "\n",
    "        return cls_targets, loc_targets\n",
    "\n",
    "\n",
    "    def encode_offsets(self, default_boxes, gt_box):\n",
    "        \"\"\"\n",
    "        バウンディングボックスのオフセットを計算する\n",
    "        \"\"\"\n",
    "        cx = (gt_box[:, 0] + gt_box[:, 2]) / 2 # (x_min + x_max) / 2 : バウンディングボックスの中心x座標\n",
    "        cy = (gt_box[:, 1] + gt_box[:, 3]) / 2  # バウンディングボックスの中心y座標\n",
    "        cx_d = (default_boxes[:, 0] + default_boxes[:, 2]) / 2 # (d_xmin + d_xmax) / 2 : デフォルトボックスのxの中心座標\n",
    "        cy_d = (default_boxes[:, 1] + default_boxes[:, 3]) / 2 # (d_xmin + d_xmax) / 2 : デフォルトボックスのyの中心座標\n",
    "        w_d = default_boxes[:, 2] - default_boxes[:, 0] # d_xmax - d_xmin : デフォルトボックスの幅\n",
    "        h_d = default_boxes[:, 3] - default_boxes[:, 1] # d_ymax - d_ymin : デフォルトボックスの高さ\n",
    "        w = gt_box[:, 2] - gt_box[:, 0] #　バウンディングボックスの幅\n",
    "        h = gt_box[:, 3] - gt_box[:, 1] #　バウンディングボックスの高さ\n",
    "\n",
    "        # 論文に従う \n",
    "        d_cx = (cx - cx_d) / (0.1 * w_d)\n",
    "        d_cy = (cy - cy_d) / (0.1 * h_d)\n",
    "        d_w = torch.log(w / w_d) / 0.2\n",
    "        d_h = torch.log(h / h_d) / 0.2\n",
    "\n",
    "\n",
    "        offsets = torch.stack([d_cx, d_cy, d_w, d_h], dim=1)\n",
    "        return offsets\n",
    "\n",
    "    def compute_cls_loss(self, cls_preds, cls_targets):\n",
    "        \"\"\"\n",
    "        クラス分類損失を計算する\n",
    "        cls_preds : [b, N, num_classes]\n",
    "        cls_targets : [b, N]\n",
    "        \"\"\"\n",
    "        #import pdb; pdb.set_trace\n",
    "        pos_mask = cls_targets > 0 # 背景以外\n",
    "        num_pos = pos_mask.sum() # 背景以外の数(バッチ全体)\n",
    "        import pdb; pdb.set_trace()\n",
    "        \n",
    "        cls_loss = F.cross_entropy(cls_preds.view(-1, self.num_classes), cls_targets.view(-1), reduction='none')\n",
    "        cls_loss = cls_loss.view(cls_targets.size()) # 各デフォルトボックス毎のロスの形にする\n",
    "\n",
    "        neg_mask = ~pos_mask\n",
    "        num_neg = min(self.neg_pos_ratio * num_pos, neg_mask.sum())\n",
    "        neg_loss = cls_loss[neg_mask].topk(num_neg, largest=False)[0].sum()\n",
    "\n",
    "        cls_loss = cls_loss[pos_mask].sum() + neg_loss\n",
    "        return cls_loss\n",
    "\n",
    "    def compute_loc_loss(self, loc_preds, loc_targets, cls_targets):\n",
    "        \"\"\"\n",
    "        オフセット損失 (Smooth L1 Loss) を計算\n",
    "        \"\"\"\n",
    "        pos_mask = cls_targets > 0 \n",
    "        num_pos = pos_mask.sum()\n",
    "\n",
    "        if num_pos == 0:\n",
    "            return loc_preds.sum() * 0\n",
    "\n",
    "        loc_loss = F.smooth_l1_loss(loc_preds[pos_mask], loc_targets[pos_mask], reduction='sum')\n",
    "        loc_loss = loc_loss / num_pos\n",
    "        return loc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfeeeb25-73eb-4f66-b87c-23ec74c2033e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "b = a.view(-1)\n",
    "print(b)\n",
    "b.view(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d289da1a-2a42-4ae3-b678-ddd9c4c13f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6439, -0.4512, -0.4485],\n",
       "         [-0.4982,  0.2407,  0.4295]],\n",
       "\n",
       "        [[ 1.6439, -0.4512, -0.4485],\n",
       "         [-0.4982,  0.2407,  0.4295]],\n",
       "\n",
       "        [[ 1.6439, -0.4512, -0.4485],\n",
       "         [-0.4982,  0.2407,  0.4295]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 基本的な使用方法\n",
    "x = torch.randn(2, 3)\n",
    "x_list = [x, x, x]\n",
    "result = torch.stack(x_list, dim=0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2152027-29e5-4834-b050-e194445e598b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0]), tensor([1]), tensor([2]), tensor([3])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.tensor([i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69bc7bb2-a6a1-4568-a5c7-060d80aafcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.tensor([i]) for i in range(4)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75bde7d3-fb6f-4229-b645-f3a00f23a214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([2])\n",
    "b = torch.tensor([1])\n",
    "torch.log(a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252262e-e0ae-4d72-9b1d-871b4f18cd87",
   "metadata": {
    "id": "a252262e-e0ae-4d72-9b1d-871b4f18cd87"
   },
   "source": [
    "### データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb840a47-a1e7-4697-b375-8f6321471f38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb840a47-a1e7-4697-b375-8f6321471f38",
    "outputId": "01a548a3-3c35-4e05-c43c-e01af7f83aff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./dataset/voc_detection\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./dataset/voc_detection\\VOCtrainval_11-May-2012.tar to ./dataset/voc_detection\n",
      "Using downloaded and verified file: ./dataset/voc_detection\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./dataset/voc_detection\\VOCtrainval_11-May-2012.tar to ./dataset/voc_detection\n",
      "Using downloaded and verified file: ./dataset/voc_detection\\VOCtrainval_11-May-2012.tar\n",
      "Extracting ./dataset/voc_detection\\VOCtrainval_11-May-2012.tar to ./dataset/voc_detection\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # 左右反転\n",
    "    transforms.RandomCrop(300, padding=8), # データの切り抜き\n",
    "    transforms.RandomRotation(10), # 回転する角度の範囲を指定. ここで10とすると、-10度から+10度までの範囲でランダムに回転\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 0~1 => -1 ~ 1\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 0~1 => -1 ~ 1\n",
    "])\n",
    "\n",
    "show_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 0~1 => -1 ~ 1\n",
    "])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for item in batch:\n",
    "        images.append(item[0])   # 画像データをリストに追加\n",
    "        targets.append(item[1])  # アノテーション（辞書）をリストに追加\n",
    "    return torch.stack(images, 0), targets  # 画像のみテンソル化し、アノテーションはリストのまま返す\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "train_dataset = VOCDetection(root='./dataset/voc_detection', year='2012', image_set='train', \\\n",
    "                       download=True, transform=train_transform)\n",
    "val_dataset = VOCDetection(root='./dataset/voc_detection', year='2012', image_set='val', \\\n",
    "                       download=True, transform=val_transform)\n",
    "show_dataset = VOCDetection(root='./dataset/voc_detection', year='2012', image_set='val', \\\n",
    "                       download=True, transform=show_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "show_loader = DataLoader(show_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a33f9-9dba-42fe-8e38-d7b65e6be557",
   "metadata": {
    "id": "747a33f9-9dba-42fe-8e38-d7b65e6be557"
   },
   "source": [
    "### まずはどのようなデータであるのか可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee2aa84d-ab73-4301-92ab-8ab9d96e8872",
   "metadata": {
    "id": "ee2aa84d-ab73-4301-92ab-8ab9d96e8872",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data augmentationしていないval_dataを用いる\n",
    "imgs, targets = next(iter(show_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d27b7868-1a4a-4dce-a959-e97a6b918163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ce01f7e-3803-4a18-86a6-b574430af6f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ce01f7e-3803-4a18-86a6-b574430af6f6",
    "outputId": "dba18e26-caf1-404c-ddc9-6b1fa04c08e5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -0.9843, -0.9922,  ..., -0.9843, -0.9843, -0.9843],\n",
       "         [-0.9922, -0.9843, -0.9922,  ..., -0.9843, -0.9843, -0.9843],\n",
       "         [-0.9922, -0.9843, -0.9922,  ..., -0.9765, -0.9922, -0.9922]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -0.9843, -0.9922,  ..., -0.9843, -0.9843, -0.9843],\n",
       "         [-0.9922, -0.9843, -0.9922,  ..., -0.9843, -0.9843, -0.9843],\n",
       "         [-0.9922, -0.9843, -0.9922,  ..., -0.9765, -0.9922, -0.9922]],\n",
       "\n",
       "        [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "         ...,\n",
       "         [-1.0000, -0.9843, -0.9922,  ..., -0.9843, -0.9843, -0.9843],\n",
       "         [-0.9922, -0.9843, -0.9922,  ..., -0.9843, -0.9843, -0.9843],\n",
       "         [-0.9922, -0.9843, -0.9922,  ..., -0.9765, -0.9922, -0.9922]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6e662484-8641-43ce-b8e3-7524488181c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e662484-8641-43ce-b8e3-7524488181c9",
    "outputId": "8a89b5eb-0982-4b68-914b-198f2fed91e9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation': {'folder': 'VOC2012',\n",
       "  'filename': '2008_000021.jpg',\n",
       "  'source': {'database': 'The VOC2008 Database',\n",
       "   'annotation': 'PASCAL VOC2008',\n",
       "   'image': 'flickr'},\n",
       "  'size': {'width': '500', 'height': '375', 'depth': '3'},\n",
       "  'segmented': '0',\n",
       "  'object': [{'name': 'aeroplane',\n",
       "    'pose': 'Frontal',\n",
       "    'truncated': '0',\n",
       "    'occluded': '0',\n",
       "    'bndbox': {'xmin': '14', 'ymin': '148', 'xmax': '475', 'ymax': '288'},\n",
       "    'difficult': '0'}]}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1354bbcf-264b-4b32-8686-3517fa53a42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_width = int(targets[1]['annotation']['size']['width'])\n",
    "img_height = int(targets[1]['annotation']['size']['height'])\n",
    "# cls_targets = torch.zeros(len(default_boxes), dtype=torch.long)\n",
    "# loc_targets = torch.zeros((len(default_boxes), 4))\n",
    "for obj in  targets[1]['annotation']['object']:\n",
    "            class_name = obj['name']\n",
    "            class_id = 500\n",
    "\n",
    "            # バウンディングボックスの正規化\n",
    "            xmin = float(obj['bndbox']['xmin']) / img_width\n",
    "            ymin = float(obj['bndbox']['ymin']) / img_height\n",
    "            xmax = float(obj['bndbox']['xmax']) / img_width\n",
    "            ymax = float(obj['bndbox']['ymax']) / img_height\n",
    "            gt_box = torch.tensor([[xmin, ymin, xmax, ymax]]) \n",
    "\n",
    "gt_box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fcc73-fd65-4fe2-8471-9edfb7a86b3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "852fcc73-fd65-4fe2-8471-9edfb7a86b3e",
    "outputId": "90cc4885-552b-47d4-d228-ee5f8a0a3532",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 可視化\n",
    "# まずは正規化を元に戻す\n",
    "img = imgs[0]\n",
    "tgt = targets[0]\n",
    "\n",
    "img = (img * 0.5) + 0.5 # -1 ~ 1 => 0 ~ 1\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc502ac-2a14-4e01-89fc-98d49415036e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "9bc502ac-2a14-4e01-89fc-98d49415036e",
    "outputId": "780cfab2-4a22-4282-c38e-0afb96cdd072",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1GRlZs5RmXhU",
   "metadata": {
    "id": "1GRlZs5RmXhU"
   },
   "source": [
    "### 学習準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "iYBjH2ehmZ4u",
   "metadata": {
    "id": "iYBjH2ehmZ4u"
   },
   "outputs": [],
   "source": [
    "ssd = SSD()\n",
    "opt = optim.Adam(ssd.parameters(), lr=0.03, weight_decay=1e-4)\n",
    "num_epochs = 10\n",
    "num_classes = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9182bb9-e401-4bee-b5d6-5d5263242372",
   "metadata": {
    "id": "f9182bb9-e401-4bee-b5d6-5d5263242372"
   },
   "source": [
    "### 学習ループ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5753ede4-6952-41ca-8277-8be6f4257bb8",
   "metadata": {
    "id": "5753ede4-6952-41ca-8277-8be6f4257bb8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def learn(model, num_epochs, optimizer, train_loader, val_loader, num_classes=21, save_path=None, early_stop=False, device='cpu'):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = SSDLoss(num_classes, model.priors, device)\n",
    "    # early stop\n",
    "    best_total_val_loss = float('inf')\n",
    "    no_update = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_total_losses = []\n",
    "        val_total_losses = []\n",
    "\n",
    "        train_loc_losses = []\n",
    "        val_loc_losses = []\n",
    "\n",
    "        train_cls_losses = []\n",
    "        val_cls_losses = []\n",
    "\n",
    "        running_train_total_losses = 0.0\n",
    "        running_val_total_losses = 0.0\n",
    "\n",
    "        running_train_loc_losses = 0.0\n",
    "        running_val_loc_losses = 0.0\n",
    "\n",
    "        running_train_cls_losses = 0.0\n",
    "        running_val_cls_losses = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for imgs, anotations in tqdm(train_loader, desc='now training', total=len(train_loader), leave=False):\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            anotations = anotations\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            lout, cout, priors = model.forward(imgs)\n",
    "            import pdb; pdb.set_trace()\n",
    "            total_loss, cls_loss, loc_loss = criterion(lout, cout, anotations)\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_total_losses += total_loss.item()\n",
    "            running_train_loc_losses += cls_loss.item()\n",
    "            running_train_cls_losses += loc_loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        for val_imgs, val_anotations in tqdm(val_loader, desc='now validation', total=len(val_loader), leave=False):\n",
    "\n",
    "            val_imgs = val_imgs.to(device)\n",
    "            val_anotations = val_anotations\n",
    "\n",
    "            val_lout, val_cout, val_priors = model.forward(val_imgs)\n",
    "            val_total_loss, val_cls_loss, val_loc_loss = criterion(val_lout, val_cout, val_anotations)\n",
    "\n",
    "            running_val_total_losses += val_total_loss.item()\n",
    "            running_val_loc_losses += val_cls_loss.item()\n",
    "            running_val_cls_losses += val_loc_loss.item()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        train_total_losses.append(running_train_total_losses / len(train_loader))\n",
    "        val_total_losses.append(running_val_total_losses / len(val_loader))\n",
    "        train_loc_losses.append(running_train_loc_losses / len(train_loader))\n",
    "        val_loc_losses.append(running_val_loc_losses / len(val_loader))\n",
    "        train_cls_losses.append(running_train_cls_losses / len(train_loader))\n",
    "        val_cls_losses.append(running_val_cls_losses / len(val_loader))\n",
    "\n",
    "        if val_total_losses[-1] < best_total_val_loss:\n",
    "                best_total_val_loss = val_total_losses[-1]\n",
    "                no_update = 0\n",
    "                if save_path is not None:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        else:\n",
    "            no_update +=1\n",
    "            if early_stop  and early_stop <= no_update:\n",
    "                break\n",
    "        print(f\"epoch {epoch+1}: train total loss {train_total_losses[-1]:.4f}, val total loss {val_total_losses[-1]:.4f}\")\n",
    "\n",
    "\n",
    "    return train_total_losses, val_total_losses, train_loc_losses, val_loc_losses, train_cls_losses, val_cls_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01cc3498-eb2b-4c98-85fa-e8252182b2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_total_losses, val_total_losses, train_loc_losses, val_loc_losses, train_cls_losses, val_cls_losses \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 2\u001b[0m learn(ssd, num_epochs, opt, train_loader, val_loader, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[65], line 34\u001b[0m, in \u001b[0;36mlearn\u001b[1;34m(model, num_epochs, optimizer, train_loader, val_loader, num_classes, save_path, early_stop, device)\u001b[0m\n\u001b[0;32m     31\u001b[0m anotations \u001b[38;5;241m=\u001b[39m anotations\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 34\u001b[0m lout, cout, priors \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(imgs)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m; pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[0;32m     36\u001b[0m total_loss, cls_loss, loc_loss \u001b[38;5;241m=\u001b[39m criterion(lout, cout, anotations)\n",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m, in \u001b[0;36mSSD.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     25\u001b[0m out, lout, cout \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m23\u001b[39m): \u001b[38;5;66;03m# 23はvggの定義でL2Normが適用されるまでに通過する層数(Conv2d, ReLU, Maxpool2d)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgg[i](X)\n\u001b[0;32m     28\u001b[0m X1 \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m     29\u001b[0m out\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL2Norm(X1)) \u001b[38;5;66;03m# out1を得る\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_total_losses, val_total_losses, train_loc_losses, val_loc_losses, train_cls_losses, val_cls_losses = \\\n",
    "learn(ssd, num_epochs, opt, train_loader, val_loader, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2d2dfc98-5245-48e4-9ed6-f2efd30a0cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = torch.randn(8, 3, 300, 300)\n",
    "lout, cout, priors = ssd.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "305c54f3-305c-4cb5-97a0-337031208d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8732, 4])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5cfd2608-9c62-49a9-a300-5453932f112c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8732, 4])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "527ed1bf-6363-4f5a-9714-3c3346482c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "craiterion = SSDLoss(21, ssd.priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4c642ee3-b8d9-4793-8998-f5daff6fcddd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "135fbd9d-3d06-45c8-95cb-d5ae343211cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32mc:\\users\\syouta\\appdata\\local\\temp\\ipykernel_14060\\1292697379.py\u001b[0m(110)\u001b[0;36mcompute_cls_loss\u001b[1;34m()\u001b[0m\n",
      "\n",
      "ipdb>  cls_preds.shape\n",
      "torch.Size([8, 8732, 21])\n",
      "ipdb>  cls_targets.shape\n",
      "torch.Size([8, 8732])\n",
      "ipdb>  cls_targets.view(-1).shape\n",
      "torch.Size([69856])\n",
      "ipdb>  cls_preds.view(-1,21)\n",
      "tensor([[-0.1450, -0.0406,  0.4179,  ..., -0.0324, -0.0599, -0.0723],\n",
      "        [ 0.0368,  0.3389, -0.6194,  ..., -0.2567, -0.3531, -0.0032],\n",
      "        [-0.0882, -0.0830,  0.0299,  ...,  0.4308, -0.2818,  0.2299],\n",
      "        ...,\n",
      "        [ 0.0068,  0.0024, -0.0216,  ...,  0.0155, -0.0022, -0.0179],\n",
      "        [ 0.0157,  0.0140, -0.0171,  ..., -0.0070, -0.0133, -0.0096],\n",
      "        [ 0.0109,  0.0177,  0.0088,  ..., -0.0084, -0.0200, -0.0132]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "ipdb>  cls_preds.view(-1,21).shape\n",
      "torch.Size([69856, 21])\n",
      "ipdb>  F.cross_entropy(cls_preds.view(-1, 21), cls_targets.view(-1), reduction='none')\n",
      "tensor([3.1950, 2.9539, 3.2010,  ..., 3.0392, 3.0257, 3.0336],\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "ipdb>  F.cross_entropy(cls_preds.view(-1, self.num_classes), cls_targets.view(-1), reduction='none')\n",
      "tensor([3.1950, 2.9539, 3.2010,  ..., 3.0392, 3.0257, 3.0336],\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "ipdb>  exit\n"
     ]
    }
   ],
   "source": [
    "craiterion(lout, cout, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0581771-c3da-4e83-9860-10f325ff413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 21, 8732])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(8, 8732, 21)\n",
    "a = a.transpose(2, 1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efb907a2-5316-449b-b102-d89ff288aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 100\n",
    "b = 10\n",
    "c = 3\n",
    "d = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a21f050-08ba-476a-8a27-8f83e4631b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8143d2eb-cbad-42f1-a494-f85c4c462211",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "397fda4e-a2c2-4cf1-80cc-2501eac505fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d / c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99b67030-c45b-414e-842b-8c10f027b0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3025850929940455"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(b / a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1f096c7-9be4-496b-a20a-0d2b51f3241b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.40546510810816444"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(d / c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ec2e1a-7969-4fd8-a21f-7ce716f67d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8732, 2]) torch.Size([8732, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(8732, 4)\n",
    "b = torch.randn(8732, 4)\n",
    "c = a[:, :2] + b[:, :2] * 0.1 * b[:, 2:]\n",
    "d = a[:, 2:] + b[:, 2:]\n",
    "print(c.shape, d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342dba23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17464, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.cat((a, b), dim=0)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b04ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
