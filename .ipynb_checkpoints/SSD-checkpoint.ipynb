{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5d7793a-1b1d-48e3-8cf2-35872e65d953",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from itertools import product\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import VOCDetection\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebe619-3fdd-4a26-abac-13dfc81d4bbd",
   "metadata": {},
   "source": [
    "### vggモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5598949-8ef2-4d2a-9cfe-5badb27df966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_vgg():\n",
    "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', \n",
    "           512, 512, 512, 'M', 512, 512, 512]\n",
    "    layers = []\n",
    "    in_chanels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2)] #出力サイズを切り捨て(デフォルトでeil_mode=False)\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, ceil_mode=True)] # 出力サイズを切り上げる\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_chanels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.ReLU()]\n",
    "            in_chanels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6, nn.ReLU(), conv7, nn.ReLU()]\n",
    "    \n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a709a49-8865-44f5-a871-30727ded823b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6660d5d6-52c8-496c-b706-156669af9692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "b = [3]\n",
    "c = [4]\n",
    "a += b\n",
    "a +=c\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e04a8fef-f64a-4e6c-9e7d-3160956a59ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3]+[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e504e64-e3c5-4b1a-a4b0-3bed8780f9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_chanels = 3\n",
    "v = 512\n",
    "layers = []\n",
    "layers += [nn.MaxPool2d(kernel_size=2)]\n",
    "pool1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "conv2 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "layers += [pool1, conv1, nn.ReLU(), conv2, nn.ReLU()]\n",
    "conv2d = nn.Conv2d(in_chanels, v, kernel_size=3, padding=1)\n",
    "layers += [conv2d, nn.ReLU()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c761abdc-4d35-40db-811f-976d21531a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False),\n",
       " Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6)),\n",
       " ReLU(),\n",
       " Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1)),\n",
       " ReLU(),\n",
       " Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8e99ca2-5bdc-4c30-b2eb-5bf7a0fdf007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.weight\n",
      "4.weight\n",
      "6.weight\n"
     ]
    }
   ],
   "source": [
    "module = nn.ModuleList(layers)\n",
    "for name, params in module.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f11e5b1-b7c2-4a1c-9278-02b0d31b211a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ModuleList(layers)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bbfe3-ef6e-4609-a4be-d61c44de1c1c",
   "metadata": {},
   "source": [
    "### extrasモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "def03c29-66c0-4adb-a2be-a933963bd9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_extras():\n",
    "    layers = [\n",
    "        # out3\n",
    "        nn.Conv2d(1024, 256, kernel_size=1), \n",
    "        nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "        # out3からout4\n",
    "        nn.Conv2d(512, 128, kernel_size=1),\n",
    "        nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "        # out4からout5\n",
    "        nn.Conv2d(256, 128, kernel_size=1), \n",
    "        nn.Conv2d(128, 256, kernel_size=3),\n",
    "        # out6からout6\n",
    "        nn.Conv2d(256, 128, kernel_size=1), \n",
    "        nn.Conv2d(128, 256, kernel_size=3),\n",
    "    ]\n",
    "    return nn.ModuleList(layers)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe90a5f-37fd-442e-8021-64a48aadbc24",
   "metadata": {},
   "source": [
    "### Locモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3cc6a2c-764d-48d0-92cb-98262a0ef70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_loc(num_classes=21):\n",
    "    \"\"\"\n",
    "    オフセットの予測を出力する\n",
    "    nn.Conv2d第2引数は出力ベクトルのchanel方向の次元数.作成されるDBox(4 or 6個)ごとにオフセットを出力するので\n",
    "    作成されるオフセットの数*4となる\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(512, 4*4, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out2に対する処理\n",
    "        nn.Conv2d(1024, 6*4, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out3に対する処理\n",
    "        nn.Conv2d(512, 6*4, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out4に対する処理\n",
    "        nn.Conv2d(256, 6*4, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out5に対する処理\n",
    "        nn.Conv2d(256, 4*4, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(256, 4*4, kernel_size=3, padding=1)\n",
    "    ]\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7f26a-254b-444c-bba7-6450d0ea47d7",
   "metadata": {},
   "source": [
    "### confモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0177d609-f3fc-4b71-9829-6479c60f1a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_coef(num_classes=21):\n",
    "    \"\"\"\n",
    "    クラスの予測を出力する\n",
    "    nn.Conv2d第2引数は出力ベクトルの次元数.作成されるDBox(4 or 6個)ごとに各クラスの信頼度を出力するので\n",
    "    作成されるオフセットの数*num_classesとなる\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(512, 4*num_classes, kernel_size=3, padding=1), # [b, 16, 38, 38]\n",
    "        \n",
    "        # out2に対する処理\n",
    "        nn.Conv2d(1024, 6*num_classes, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out3に対する処理\n",
    "        nn.Conv2d(512, 6*num_classes, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out4に対する処理\n",
    "        nn.Conv2d(256, 6*num_classes, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out5に対する処理\n",
    "        nn.Conv2d(256, 4*num_classes, kernel_size=3, padding=1),\n",
    "        \n",
    "        # out1に対する処理\n",
    "        nn.Conv2d(256, 4*num_classes, kernel_size=3, padding=1)\n",
    "    ]\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0772b-33a5-4443-890f-280efc1824cb",
   "metadata": {},
   "source": [
    "### L2Normの実装\n",
    "### Layer Normalizationの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d916f6-3308-40d6-ac78-e70cc434051e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.6923e-15,  1.4419e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf25be18-dc01-40b0-9d12-ee330ca5e710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18466715-c13b-4a33-8228-01dc43edf4fa",
   "metadata": {},
   "source": [
    "###### nn.Parameterは、PyTorchのモデル内で学習可能なパラメータを定義するために使用されるクラスです。主に、ニューラルネットワークの層で使用され、バックプロパゲー ションを通じて最適化される重みやバイアスを定義します。\n",
    "\n",
    "###### nn.init.constant_は、PyTorchの初期化モジュールで提供される関数の一つで、テンソルのすべての要素を指定した定数で初期化するために使用されます。この関数は、通常、モデルのパラメータの初期値を設定する際に利用されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee5882-a1d8-4b1a-a54b-153afac1aae4",
   "metadata": {},
   "source": [
    "#### L2Norm : チャネル方向のL2Normの合計を1にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8c49a2c-7a4b-44a5-9e12-f9a66d99ab7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, n_channels=512, scale=20):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.gamma = scale # 正規化後に掛けるパラメータ,channel分だけある.(これはbackwaradで最適化される)\n",
    "        self.eps = 1e-10 # 0で割ることを防ぐためのε\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self): \n",
    "        nn.init.constant_(self.weight, self.gamma) # self.gamma(デフォルトで20)でweightを初期化\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X : [b * c * h * w]を想定\n",
    "        \"\"\"\n",
    "        norm = X.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps # norm : [b, 1, h, w]\n",
    "        \n",
    "        # 入力をnormで割る\n",
    "        X = torch.div(X, norm) # X : [b, c, h, w]\n",
    "        \n",
    "        # スケーリングの重みを掛ける\n",
    "        out = self.weight.reshape(1, self.n_channels, 1, 1) * X # self.weight.reshape(1, self.n_channels, 1, 1) : [1, c, 1, 1]\n",
    "                                                                # out : [b, c, h, w]\n",
    "       \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1301af03-6dff-4dcb-b645-892044248df5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = nn.Parameter(torch.Tensor(3))\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c27c410b-0e5a-4bb4-a5a4-93e529fc27d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight = nn.Parameter(torch.Tensor(3)) \n",
    "eps = 1e-10\n",
    "X = torch.randn((8, 3, 8, 8))\n",
    "norm = X.pow(2).sum(dim=1, keepdim=True).sqrt() + eps\n",
    "\n",
    "X = torch.div(X, norm) # これで各ピクセルの和は1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0264f63b-9a8e-407d-8e77-aa4ee55e9dca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 8, 8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.shape # データ1つの正規化(channel間での正規化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46da56a9-eba7-42bd-87ac-c37097ebb23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 8, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6b56c71-53bd-46fd-a873-4d3a955f1a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(X) * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "004b53a3-10dc-4351-a9fd-439e0898e696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 8, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd2e20-d6a3-4305-b77d-08ba4c848cf1",
   "metadata": {},
   "source": [
    "### DBoxの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a7d1f-c68a-4873-b20a-20027473d57c",
   "metadata": {},
   "source": [
    "###### self.min_sizes = [30, 60, 111, 162, 213, 264]\n",
    "###### self.max_sizes = [60, 111, 162, 213, 264, 315]\n",
    "###### この値から、解像度が38*38の特徴マップでは画像の縦、横10% ~ 20%の大きさの画像の検出が得意ということ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a7fa67b-5923-4f53-9b33-a74f80eab183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PriorBox:\n",
    "    def __init__(self):\n",
    "        self.image_size = 300 # 入力画像のサイズを300 × 300と想定\n",
    "        # 解像度が38 : 1つの特徴量マップでは300/38で7ピクセル分の情報を表現\n",
    "        self.feature_maps = [38, 19, 10, 5, 3, 1] \n",
    "        self.steps = [8, 16, 32, 64, 100, 300] # 特徴量マップの1セルが何ピクセルを(ピクセル/セル)表現するかをリストに格納.32, 64は計算の効率性のため少しずらした値(2の累乗)を設定\n",
    "                                               # 例えば300/38 ≒ 8, 300/19 ≒ 16としている\n",
    "        self.min_sizes = [30, 60, 111, 162, 213, 264] # 30 ... 画像の10%程度の大きさの物体の検出に適している \n",
    "        \n",
    "        self.max_sizes = [60, 111, 162, 213, 264, 315] # 60 ...画像の20%程度の大きさの物体の検出に適している \n",
    "        self.aspect_rations = [[2], [2, 3], [2, 3], [2, 3], [2], [2]] \n",
    "        \n",
    "    def forward(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps): # [38, 19, 10, 5, 3, 1] \n",
    "            for i, j in product(range(f), repeat=2): # 各特徴量マップのセルごとにDBox作成\n",
    "                #self.steps は計算効率のために調整されたセルのピクセル数を格納しているが、\n",
    "                # 実際のDBoxの配置で精度を保つために、f_k = self.image_size / self.steps[k] で再度スケールを計算\n",
    "                f_k = self.image_size / self.steps[k] # 特徴量マップf_k個で1になる\n",
    "                cx = (j + 0.5) / f_k # 比の計算 これを座標としている\n",
    "                cy = (i + 0.5) / f_k\n",
    "                s_k = self.min_sizes[k] / self.image_size # これは最小サイズの正方形のサイズ. 特徴量マップの解像度で固定\n",
    "                mean += [cx, cy, s_k, s_k] # 最小サイズの正方形のDBox作成\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) # これは最大サイズの正方形のサイズ. 特徴量マップの解像度で固定\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime] # 最大サイズの正方形のDBox作成\n",
    "                for ar in self.aspect_rations[k]:\n",
    "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
    "        output = torch.Tensor(mean).view(-1, 4) \n",
    "        # イメージ : [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4] \n",
    "        # ===>\n",
    "        # tensor([[1., 2., 3., 4.],\n",
    "        # [5., 6., 7., 8.],\n",
    "        # [1., 2., 3., 4.]])\n",
    "\n",
    "        output.clamp_(max=1, min=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a294e3a-60bf-4b77-9a62-e14c588140d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "300/8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7eeeb794-c18f-4c07-97bf-f7e5bad20af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# from itertools import product\n",
    "for i, j in product(range(3), range(4)): # 直積を計算\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec102f09-8bf6-4d0e-b672-0dc5b72d2e69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "2 0\n",
      "2 1\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "for i, j in product(range(3), repeat=2):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "155be06b-d720-45d2-a895-8ae4f26288d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = []\n",
    "mean += [1,2,3,4]\n",
    "mean += [5,6,7,8]\n",
    "mean += [1,2,3,4]\n",
    "# エラーになる\n",
    "# mean += [1,2,3]\n",
    "\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18172f55-a14e-4a68-8842-2e75c4d1e196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.],\n",
       "        [1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(mean).view(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34588fef-205a-4151-8eee-b664dfa37b62",
   "metadata": {},
   "source": [
    "### SSDのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43de89ef-f9f8-4036-9cd3-cd149d89e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self, phase='train', num_classes=21):\n",
    "        super().__init__()\n",
    "        self.phase = phase\n",
    "        self.num_classes = num_classes\n",
    "        self.vgg = make_vgg()\n",
    "        self.extras = make_extras()\n",
    "        self.L2Norm = L2Norm()\n",
    "        self.loc = make_loc()\n",
    "        self.conf = make_coef()\n",
    "        dbox = PriorBox()\n",
    "        self.priors = dbox.forward() # self.priorsには各解像度の各セルに対してのDBoxが4or6個格納されている\n",
    "        \n",
    "        if phase == 'test':\n",
    "            self.detect = Dtect()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X : [b, c, h=300, w=300]\n",
    "        \"\"\"\n",
    "        bs = X.shape[0]\n",
    "        # lout = []  各セルのオフセットの予測が格納\n",
    "        # cout = []  各セルのクラス分類の結果が格納\n",
    "        # out = []  各解像度の特徴マップが出力\n",
    "        out, lout, cout = [], [], []\n",
    "        for i in range(23): # 23はvggの定義でL2Normが適用されるまでに通過する層数(Conv2d, ReLU, Maxpool2d)\n",
    "            X = self.vgg[i](X)\n",
    "        X1 = X\n",
    "        out.append(self.L2Norm(X1)) # out1を得る\n",
    "        \n",
    "        for i in range(23, len(self.vgg)):\n",
    "            X = self.vgg[i](X)\n",
    "            \n",
    "        out.append(X) # out2を得る\n",
    "        \n",
    "        # out3,4,5,6\n",
    "        for i in range(0, 8, 2):\n",
    "            X = F.relu(self.extras[i](X))\n",
    "            X = F.relu(self.extras[i+1](X))\n",
    "            out.append(X)\n",
    "        \n",
    "        # オフセットとクラス毎の信頼度を求める\n",
    "        for i in range(6): # out1~out6に対する出力処理\n",
    "            # 各セルのオフセットの予測\n",
    "            lx = self.loc[i](out[i]).permute(0,2,3,1).reshape(bs, -1, 4)\n",
    "            # self.loc[i](out[i]).permute(0,2,3,1) : [bs, 38, 38, 16] reshape後 : [bs, 38*38*4, 4]になるのでは...\n",
    "            # 書籍では[bs, 38*38, 4] と各セルに対してオフセットが得られるとあるが...多分誤植\n",
    "            \n",
    "            # cout = []  各セルのクラス分類を予測\n",
    "            cx = self.conf[i](out[i]).permute(0,2,3,1).reshape(bs, -1, self.num_classes)\n",
    "            lout.append(lx)\n",
    "            cout.append(cx)\n",
    "        #import pdb; pdb.set_trace()  \n",
    "        lout = torch.cat(lout, 1) # [bs, 38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4, 4] # 1枚の画像の38*38の各セルに対して4つのDBoxがあり,4次元のオフセットがある\n",
    "        cout = torch.cat(cout, 1) # [bs, 38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4, self.num_classes] # 1枚の画像の38*38の各セルに対して4つのDBoxがあり, 21クラスのクラス分類を行う\n",
    "        outputs = (lout, cout, self.priors)\n",
    "        if self.phase == 'test':\n",
    "            return self.detect.apply(output, self.num_classes)\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90da8690-0bae-4c96-92eb-cb96080df955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8732"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38*38*4+19*19*6+10*10*6+5*5*6+3*3*4+1*1*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d7468ef6-cb2c-4b73-bce4-bf3e587e4973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tensor = torch.randn(8, 3, 300, 300)\n",
    "test_model = SSD()\n",
    "lout, cout, priors = test_model(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6d619c7e-4e7e-4e9e-a74b-5b34e7c2fb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8732, 4])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各セルにおけるオフセット\n",
    "lout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8648d3a3-4b9f-4a50-b279-5f77b8a25cfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8732, 21])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各セルにおけるクラス分類\n",
    "cout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc6bca15-3b0c-4b4b-833f-fd66f36c0f91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8732, 4])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各セルにおけるDBox\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252262e-e0ae-4d72-9b1d-871b4f18cd87",
   "metadata": {},
   "source": [
    "### データ準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "eb840a47-a1e7-4697-b375-8f6321471f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # 左右反転\n",
    "    transforms.RandomCrop(300, padding=8), # データの切り抜き\n",
    "    transforms.RandomRotation(10), # 回転する角度の範囲を指定. ここで10とすると、-10度から+10度までの範囲でランダムに回転\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 0~1 => -1 ~ 1\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 0~1 => -1 ~ 1\n",
    "])\n",
    "\n",
    "dataset = VOCDetection(root='./dataset/voc_detection', year='2012', image_set='train', \\\n",
    "                       #download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9182bb9-e401-4bee-b5d6-5d5263242372",
   "metadata": {},
   "source": [
    "### 学習ループ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753ede4-6952-41ca-8277-8be6f4257bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
